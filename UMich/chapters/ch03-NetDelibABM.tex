\section{Introduction}

Deliberation enables the generation, aggregation, and synthesis of diverse knowledge \cite{dewey_creative_1940, anderson_epistemology_2006, ackerman_deliberation_2002, ostrom_collective_2000},
as well as the early identification and resolution of conflict \cite{gentry_consensus_1982}.
As a participatory process, deliberation builds trust among participants, increases the perception of fairness, and incentivises cooperation with group decisions
\cite{ostrom_collective_2000, bowles_endogenous_1998}.
As a dynamic process, deliberation alters private preferences through discourse \cite{habermas_structural_1991}, sidestepping fundamental limitations of voting, e.g., the Condorcet paradox \cite{condorcet_essay_1785, brandt_computational_2012} and Arrow's impossibility theorem \cite{arrow_social_2012, brandt_computational_2012}.
Despite these benefits, deliberation becomes prohibitively difficult in larger groups, due to the increased time and effort needed to reach decisions \cite{fishkin_voice_1997, gentry_consensus_1982} and to the emergence of power inequalities \cite{freeman_tyranny_1972, boehm_egalitarian_1993}.
Yet examples of successful mass deliberation do exist, including Wikipedia \cite{giles_internet_2005, keegan_evolution_2017}, free and open source software projects \cite{benkler_coases_2002}, grassroots protest and crisis-response networks \cite{gonzalez-bailon_networked_2016, brugh_combining_2019} and self-managed organizations \cite{laloux_reinventing_2014}.
The commonalities between such projects provide insight into overcoming the challenges of mass deliberation.
Specifically, we evaluate a model of mass deliberation inspired by common network features observed in successful projects.

Communication in large collaborations is necessarily restricted when group size exceeds individual members' capacity for communication. The structure of the resulting communication network is an important factor in the success of collaborative tasks \cite{kearns_experiments_2012, mason_collaborative_2012, mason_propagation_2008}, such as deliberation. Among successful mass deliberative projects, communication networks often exhibits a common structure: small interlocking groups \cite{benkler_coases_2002, gonzalez-bailon_networked_2016, brugh_combining_2019, laloux_reinventing_2014}. A similar structure has been observed in the interlocking directorates of corporate boards \cite{karau_social_1993} and the interlocking publics of the public sphere \cite{habermas_structural_1991}, as well as utilized for collective design in network rotation \cite{salehi_hive_2018}.
These interlocking groups have been given various names: committees, working groups, projects, modules, affinity groups, circles, teams, cores, nodes, zones, cells, or syndicates.
We shall use the term {\em pods} to encompass all groups exhibiting two key characteristics: small enough to enable deliberation, and interlocking to enable information diffusion.
We shall use the term {\em network deliberation} to refer to mass deliberation using an interlocking pod structure.
Communication network structure also influences the success of collaborations by limiting the maximum speed of information diffusion \cite{lazer_network_2007,mason_propagation_2008,mason_collaborative_2012,barkoczi_social_2016, gomez_clustering_2019,derex_partial_2016,kearns_experiments_2012}.
Structurally efficient networks,
those with typically short paths between individuals,
enable fast diffusion and favor exploitation of known information over exploration of the unknown \cite{lazer_network_2007, barkoczi_social_2016}. In the context of network deliberation, structural efficiency is determined by how individuals are assigned to pods.
We will focus on two such methods: one efficient, one inefficient.

Collective tasks such as deliberation are influenced not only by communication structure, but also by individual behavior. Individuals weigh social influence against their personal knowledge and expertise  \cite{boyd_culture_1988}. 
Even in a collective setting, individuals sometimes work independently to search for new and innovative ideas \cite{lazer_network_2007, barkoczi_social_2016}, to utilize their unique information and perspectives \cite{hong_interpreted_2009}, and to critically evaluate the ideas of others \cite{rendell_rogersparadox_2010}.
Alternatively, individuals can defer to others in order to exploit and propagate good ideas \cite{barkoczi_social_2016, boyd_culture_1988}, to conserve their own resources through free-riding and social loafing \cite{karau_social_1993}, or due to a high level of trust \cite{salehi_hive_2018} or social influence \cite{banerjee_simple_1992, smith_pathological_2000}. Individual behavior interacts with network structure to vary the level of exploration of new ideas versus the exploitation of known information \cite{barkoczi_social_2016}. While the focus of network deliberation is structural, the effect of network structure must be understood in the context of individual behavior and social dynamics.

We apply a formal social learning framework to model network deliberation.
Specifically, we address two questions. How effective is network deliberation compared to conventional single-group deliberation? And how do network efficiency and individual behavior influence the outcome of network deliberation?
We present the following three contributions.
First, network deliberation out-performs conventional deliberation in both solution speed and quality when agents rely heavily on social learning. This suggests that network deliberation is a promising deliberation design for groups with a high degree of social influence. Second, network deliberation performs best on structurally efficient networks when agents exhibit conformist behavior, while it performs best on structurally inefficient networks when agents greedily maximize solution quality, consistent with findings for conventional networks \cite{barkoczi_social_2016}.
Third, the naive greedy strategy can be modified to improve performance on conventional networks despite using strictly less information. In the case of network deliberation, the naive and modified greedy strategies are equivalent.

\section{Methods}

Deliberation can be modeled as a form of collective problem-solving and social learning in which agents hold private beliefs, share information with their neighbors, and update their beliefs based on a combination of individual analysis and social influence.
Following existing models of collective problem-solving \cite{lazer_network_2007, barkoczi_social_2016, gomez_clustering_2019}, our model comprises: 1. a collective task, 2. a network, and 3. a learning strategy.
Agents maintain a candidate solution to the task and iteratively apply their learning strategy to the available information, yielding an updated candidate.


\subsection{Overview}

As their collective task, agents seek an optimal point on a rugged fitness landscape.
This landscape is generated using the NK model \cite{kauffman_towards_1987, weinberger_local_1991} (see Appendix \ref{apx:nk}).
The NK model is a ``tuanbly rugged'' objective function parameterized by two variables:
the number of bits in the input string ($N$),
and the number bits used to compute each contribution to the objective function ($K+1$).
These parameters can be used to tune problem size and complexity, respectively.
The resulting objective function maps $N$-bit strings to real numbers in $[0,1]$.

To model network deliberation, we construct interlocking networks in stages. At each stage, agents are partitioned into small {\em pods} and connected to all other agents in the same pod.
Our choice of pod size is informed by studies of real-world group dynamics.
The upper limit of effective small group size has been estimated variously as five \cite{freeman_tyranny_1972}, five--eight\cite{lohman_designing_2000}, and eight \cite{miflin_small_2004}.
We choose a pod size of 5.
Different methods for assigning agents to pods produce networks with different properties.
Interlocking structure is created when agents are placed with others they have not yet interacted with.
We thus require that pod assignment methods satisfy the following mixing property:
\begin{property}
\label{prop:mixing}
For any agent $v$, and any two assignment stages $t$, $t^\prime$, the probability that at least one neighbor of $v$ at stage $t$ is also a neighbor at stage $t^\prime$ approaches 0 as $|V|$ grows.
\end{property}
We study two such methods: a random assignment method that produces structurally efficient networks, and a long-path method that produces inefficient networks. For comparison, we also simulate conventional deliberation. While the potential communication links in conventional deliberation form a fully connected network, actual communication is better modeled by preferential attachment \cite{barabasi_emergence_1999} or small-world \cite{watts_collective_1998} networks (see Methods).

An agent's learning strategy models both informational constraints and behavioral dynamics \cite{lazer_network_2007, barkoczi_social_2016} such as social influence and social loafing \cite{karau_social_1993}.
We consider three social learning strategies.
The {\em best-neighbor} strategy is the naive greedy strategy: agents simply adopt the neighboring solution with the highest value. This strategy requires that agents have the ability to compare the quality of two arbitrary solutions, a strong assumption representing either skilled agents, or a simple task.
The {\em conform} strategy is pure social influence. Agents adopt the most common solution among their neighbors.
This strategy requires no knowledge of solution quality, modeling contexts where agents are unable to compare solution quality, or where agents choose not to, as in social loafing \cite{karau_social_1993}.
We also introduce the {\em confident-neighbor} strategy, a variant of best-neighbor in which agents announce when they have the best solution in their neighborhood. This variant makes weaker assumptions about agent ability but is equivalent to best-neighbor when network deliberation is used.
This strategy allows us to compare low-social influence and high-social influence strategies under equivalent assumptions of agent ability.

Between each stage of social learning, agents may perform individual learning. Following existing literature \cite{lazer_network_2007, barkoczi_social_2016}, we model individual learning by mutating a single bit of an agent's solution, and keeping it if the mutation improves solution quality.
A complete learning strategy must also specify how social and individual learning are integrated. Existing literature \cite{lazer_network_2007, barkoczi_social_2016} applies individual learning only when social learning fails to produce an improved solution. We refer to this method as {\em fallback} individual learning. Fallback can be also be interpreted as a model of social loafing \cite{karau_social_1993}. For completeness, we include analysis for a second method in which agents first perform social and individual learning in {\em parallel} and then choose the better of the two learning outputs.

\subsection{Agent-Based Model}

We use an agent-based simualation to model deliberation between individuals.
We model communication between agents using a time-dependent network $(V,E_t)$.
The vertices $V$ correspond to the agents (individuals).
The edges $E_t$ allow agents to exchange information with their immediate neighbors at time $t$.
Over the course of the simulation, agents seek to maximize some objective function $Q(s)$.
We use binary strings of length $d$ as our solution space: $s \in \mathbb{Z}^d$.
We generate the objective function $Q(s)$ using the NK model \cite{kauffman_towards_1987, weinberger_local_1991} (see Appendix \ref{apx:nk}).

The simulation begins at time $t=0$ by generating a set of initial solutions $s_{v,0}$ for the agents.
These are generated randomly, with each possible solution having equal probability.
The simulation proceeds iteratively.
At time $t$ each agent applies one of several {\em learning strategies}, to determine its preferred solution at time $t+1$.
Learning strategies can rely on the agent's own solution, the solutions of its neighbors, and potentially additional information shared by its neighbors.
Learning strategies may also incorporate information about the objective function, modeling an agents' ability to evaluate the quality of solutions.
To allow for ties, learning strategies produce a set of solutions rather than a single solution.
In our simulations, we choose winners at random in the case of a tie.
We present results for 1000 randomly generated NK model objectives.
with each network/strategy combination simulated once per objective function.
In keeping with other authors \cite{barkoczi_social_2016, lazer_network_2007} we use $N=15$ and $K=6$.
We allow each simulation to proceed for 300 iterations, which we have found sufficient to guarantee convergence with our chosen parameters.
For real-world comparison, on average, an active English-language Wikipedia editor makes about 45 edits per month \cite{noauthor_wikipediastatistics_2022}.

\subsection{Networks}

We carry out simulations for four different network topologies. In addition to the well-known preferential-attachment \cite{barabasi_emergence_1999} and small-world \cite{watts_collective_1998} networks, we construct two types of networks to model network deliberation.
Both types exhibit interlocking structure and satisfy Property \ref{prop:mixing}.
These two network deliberation conditions differ in how agents are assigned to pods and in the structural efficiency of the resulting network.
At each iteration, existing edges are replaced with cliques corresponding to the pods. An edge is created between two agents if and only if they share a pod.
In {\em random-pod} assignment, agents are assigned to pods at random.
This assignment method produces short geodesic paths and efficient network structure.
In {\em long-path} assignment, agents are assigned using an algorithm which guarantees interlocking structure while preventing the creation of long-distance ``shortcut'' edges, producing a structurally inefficient network.
All networks are constructed with 100 vertices/agents and a mean degree of 4 (pod size of 5).

\subsection{Conventional Deliberation}
We model the communication network structure of conventional deliberation using two static networks: small-world networks \cite{watts_collective_1998}, and preferential attachment networks \cite{barabasi_emergence_1999}. In typical deliberative settings, from deliberative assemblies to online forum threads, the contributions of any member are potentially visible to all other members.
The network of {\em potential} communication can be modelled by a complete graph.
However, a complete graph may not be the best model for the {\em actual} communication that takes place in such a network. In a real-world deliberation, some participants are more influential than others \cite{goel_structure_2012, shaw_laboratories_2014}, and many communications can be missed or ignored by any particular participant.

Human social networks often exhibit large clustering, and short path lengths.
To model such dynamics, we use Watts-Strogatz small-world networks  \cite{watts_collective_1998}.
These networks model, for example, when participants interact mostly with their strong ties, but occasionally with a weak tie. Social networks have also been found to exhibit skewed degree distributions with long tails \cite{barabasi_emergence_1999}.
We model these networks using the BarabÃ¡si-Albert preferential attachment model.
These networks model settings where a small number of individuals produce a disproportionate share of the communication due to factors such as social capital, expertise, or confidence.

\subsection{Network Deliberation: Interlock Networks}
Both of our network deliberation conditions use networks built from interlocking pods.
These pods are small complete graphs of roughly equal size.
At any given time, each agent can communicate with all other agents in exactly one pod.
Periodically, all agents are simultaneously reassigned to new pods.
These reassignments create an interlocking structure, allowing information to flow through the entire group.
This structure can be represented as a time-dependent or directed network.
The properties of an interlock network depend on the particular method for assigning agents to pods.
Crucially, interlocking structure is created when agents are placed with others they have not yet interacted with.
We thus require that pod assignment schemes satisfy the mixing property (Property \ref{prop:mixing}).

\subsubsection{Random-Pod Assignment}
The {\em random-pod} assignment method (Algorithm \ref{alg:random}) simply assigns agents to groups at random. Pseudocode for this method is shown in Algorithm \ref{alg:random}.

\begin{algorithm}
\SetAlgoLined
\DontPrintSemicolon
\SetKwFunction{removeRandom}{removeRandom}
\SetKwFunction{insert}{insert}
\SetKwFunction{emp}{empty}
\SetKw{algnot}{not}
\KwData{A vertex list $V$, the pod size $M \in \mathbb{Z}$.}
\KwResult{A partition of the vertices.}
    $N \longleftarrow \lceil \frac{|V|}{M} \rceil$ \;
    $P \longleftarrow$ List of $N$ empty sets \;
    \For{$i \in 1, \ldots, M$}{
        \ForEach{$s \in P$}{
            \If{\algnot $V.\emp()$}{
                $v \longleftarrow V.\removeRandom()$ \;
                $s.\insert(v)$ \;
            }
        }
    }
    \Return{$P$}
\caption{Random-Pod Assignment}
\label{alg:random}
\end{algorithm}

\begin{claim}
Random-pod assignment satisfies Property \ref{prop:mixing}.
\end{claim}

\begin{proof}
Let $v$ be a vertex of graph $G=(V,E_t)$ with a set of $M$ neighbors at stage $t$ denoted $N_t(v)$.
The probability that the $k$th neighbor chosen at time $t^\prime \neq t$ belongs to $N_t(v)$, given that the first $k - 1$ did not, is:
\begin{align*}
p_\text{repeat}(k)
&= \frac{M - 1}{|V| - k} \\
&\leq \frac{M - 1}{|V| - M}.
\end{align*}
The probability that none of the $M-1$ neighbors chosen at time $t^\prime$ belong to $N(v)$ thus satisfies:
\begin{align*}
p_\text{mixed}
&\geq
\left( 1 - \frac{M - 1}{|V| - M} \right)^{M-1} \\
\lim_{|V|\rightarrow\infty} p_\text{mixed}
&= 1
&\qedhere
\end{align*}
\end{proof}

Random pod assignment also creates structurally efficient networks with short geodesic distances, as shown in Figure \ref{fig:broadcast}.
There has been evidence both for
\cite{lazer_network_2007, derex_partial_2016, mason_propagation_2008, barkoczi_social_2016}
and against
\cite{mason_collaborative_2012, barkoczi_social_2016}
the effectiveness of structurally efficient networks in social learning.

\begin{figure}
    \centering
    \includegraphics[width=3.33in]{chapters/figures/NetDelibABM/fig-broadcast.eps}
    \caption{Mean time necessary for a signal broadcast from one node to reach the entire network. We use mean broadcast time as a measure of geodesic length for time-varying networks.}
    \label{fig:broadcast}
\end{figure}


\subsubsection{Long-Path Pod Assignment}

To study the effects of path length, we require an alternative pod assignment method that produces long paths.
However, we must still retain Property \ref{prop:mixing} such that it is rare for two agents to repeatedly share the same pod.
This property ensures the creation of many new edges at each stage, which is difficult--but not impossible--to reconcile with long average path length.
We now present a {\em long-path} pod assignment method, which meets both of the above goals.

We begin with a high-level overview.
Agents are assigned an integer position on a 1-dimensional circular lattice.
By preferring short-distance links on this lattice, we maintain long path lengths.
We also partition agents according to the remainder of their position, modulo some prime, i.e., their residue class.
By limiting links to agents in the same residue class, and using a unique prime at each stage, we ensure that it is rare for multiple agents to share a pod twice in a row.
Specifically, for stages using primes $p$ and $q$, two agents will share a pod for both stages when their positions are equal modulo $pq$. Pseudocode for long-path assignment is shown in Algorithm \ref{alg:long}.

\begin{algorithm}
\SetAlgoLined
\DontPrintSemicolon
\SetKw{algnot}{not}
\SetKwFunction{removeRandom}{removeRandom}
\SetKwFunction{moduli}{moduli}
\SetKwFunction{append}{append}
\SetKwFunction{enqueue}{enqueue}
\SetKwFunction{dequeue}{dequeue}
\SetKwFunction{concat}{concatenate}
\KwData{A vertex list $V$, the pod size $M \in \mathbb{Z}$, the stage $t \geq 0 \in \mathbb{Z}$, a list of co-prime integers $\moduli$.}
\KwResult{A partition of the vertices.}
    $N \longleftarrow \lceil \frac{|V|}{M} \rceil$ \;
    $P \longleftarrow$ List of $N$ empty sets \;
    \eIf{$t = 0$}{
        // Place all vertices in same residue class \;
        $p \longleftarrow 1$ \;
    }{
        // Choose integer to define residue class \;
        $p \longleftarrow \moduli[t]$ \;
    }
    // Assign vertices to residue classes \;
    $R \longleftarrow$ List of $p$ empty queues \;     
    \For{$z \in 0, \ldots, |V|-1$}{
        $R[z \bmod p].\enqueue(V[z])$ \;
    }
    // Divide each residue class into pods \;
    $P \longleftarrow $ Empty list of lists. \;
    \For{$r \in R$}{
        $N_r \longleftarrow \lceil \frac{|r|}{M} \rceil$ \;
        $P_r \longleftarrow$ List of $N_r$ empty lists \;
        \For{$i \in 1, \ldots, M$}{
            \For {$j \in 0, \ldots, N_r - 1$}{
                \If{ \algnot $r.\emp()$}{
                    $v \leftarrow r.\dequeue()$ \;
                    $P_r[j].\append(v)$ \;
                }
            }
        }
        $P.\concat(P_r)$
    }
    \Return{$P$}
\caption{Long-Path Assignment}
\label{alg:long}
\end{algorithm}


\begin{claim}
When the pod size $M$ is less than the modulus $p_t$ for all stages $t$, long-path pod assignment satisfies Property \ref{prop:mixing}.
\end{claim}

\begin{proof}
Let $p$ and $q$ be co-prime integers used as moduli for stages $t$ and $t^\prime$ respectively.
Let $z(x)$ denote the integer position of vertex $x$.
Let $v$ be a vertex, and let us assume vertex $w$ belongs to the same pod as $v$ at both time $t$ and $t^\prime$.
This assumption implies:
\begin{align*}
    z(v) &\equiv z(w) \pmod{p} \\
    z(v) &\equiv z(w) \pmod{q} \\
    \implies z(v) - z(w) &\equiv 0 \pmod{p} \\
    &\equiv 0 \pmod{q} \\
    &= npq & \text{for some integer n}.
\end{align*}
By the definition of the long-path algorithm:
\begin{align*}
    |z(v)-z(w)| & \leq (M - 1)p \\
    \implies |n|pq & \leq (M - 1)p \\
    |n|q & \leq (M - 1).
\end{align*}
By assumption, $q \geq M$, so the above can only be satisfied by $n=0$,
which in turn implies $v=w$. \qedhere
\end{proof}

To see that the long-path procedure produces long geodesics, note that the pods of size $M$ connect nodes at most $(M - 1)p_i$ apart in position, preventing the creation of any shortcut edges, as long as $p_i$ is small compared to $|V|$.
Numerical simulations confirm that the long-path algorithm produces geodesics larger than random pod assignment (Figure \ref{fig:broadcast}).
The number of agents may not be an exact multiple of $M$, so the final pods may be truncated. However, the network will be a sub-graph of a network that does have a multiple of $M$ nodes, so the structure will not be fundamentally changed, and these edge effects should become negligible as the number of agents increases.


\subsection{Learning Strategies}
The choice of network structure defines which agents can exchange information, but not how those agents act on the information received. An agent's actions based on available information are determined by that agent's {\em learning strategy}. Learning strategies can be divided into {\em individual learning strategies} which use only information directly observable by the agent, and {\em social learning strategies} which use information communicated by neighboring agents.
Generally, a learning strategy may incorporate both social and individual components.
In this paper, we consider three separate strategies: {\em best-neighbor}, {\em confident-neighbor}, and {\em conform}.
Each is combined with the same individual learning strategy.

The social learning strategies available to an
individual depend on that individual's ability to evaluate the quality of candidate solutions.
Individuals with less information about solution quality, must rely
more heavily on the recommendations and influence of their peers.
We now develop a formalism describing an agent's ability to evaluate the quality of a candidate solution.
This formalism provides precise language for distinguishing between behavioral and informational constraints on agent behavior.

\subsubsection{Agent Capabilities}
\label{sec:capability}
In human social learning, the ability of individuals to evaluate the quality of a solution can vary with factors such as expertise and task type. When comparing learning strategies, it will be helpful to classify them according to the capabilities they require of agents. We formalize these {\em capabilities} as oracle functions, which accept one or more task solutions as input and reveal whether the states satisfy some particular property.

One of the strongest capabilities an agent might posses, is to compare two arbitrary solutions to determine which yields a higher value of the objective function. We represent this capability using the {\em arbitrary comparison} oracle.
\begin{definition}
The arbitrary comparison oracle $\mathcal{O^>}(s_1, s_2)$ is given by:
\begin{eqnarray}
\mathcal{O}^>(s_1, s_2) &\equiv&
Q(s_1) > Q(s_2)
\end{eqnarray}
\end{definition}

Under a weaker assumption, agents might be ``experts'' on their current solution and able to compare that particular solution to any other. This ability is represented by a {\em single-solution comparison} oracle.
\begin{definition}
The single-solution comparison oracle for solution $s$ is given by:
\begin{eqnarray}
\mathcal{O}^>_s(\tilde{s}) &\equiv&
Q(\tilde{s}) > Q(s)
\end{eqnarray}
\end{definition}

In many contexts, it is reasonable to assume that agents can explore the effects of small changes to their current solution. When the solutions are binary strings, such variations can be modeled by flipping a single bit of the solution string. We formalize this capability through the {\em mutation comparison} oracle.
\begin{definition}
The mutation comparison oracle for solution $s$ is given by:
\begin{eqnarray}
\mathcal{O}^\oplus_s(i) &\equiv&
Q(s \oplus e_i) > Q(s),
\end{eqnarray}
where $\oplus$ represents addition mod 2 and $e_i$ represents the binary string with a single 1 at index $i$.
\end{definition}

\subsubsection{Mutation-Based Individual Learning}
In real-world collaborations, individuals sometimes work independently, even when communication is available. Independent work might be motivated by practical concerns (such as distributing labor) or by social dynamics. We model individual learning using single-bit mutations, following the examples of
\cite{lazer_network_2007} and \cite{barkoczi_social_2016}. Agents constructs a candidate solution by flipping a single bit of their solution at an index selected uniformly at random. If the candidate solution yields a higher value of the objective function, the agent adopts it as the new solution. As this strategy requires comparing solutions that differ by at most one bit, an oracle at least as powerful as the mutation comparison oracle is required.

\begin{definition}
The mutation individual learning strategy $\mathcal{L}_{\text{I}}(v)$ is defined as:
\begin{eqnarray}
i &\sim& \unif(1,d) \nonumber\\
\mathcal{L}_{\text{I}}(v) &\equiv&
\begin{cases}
\{s(v) \oplus e_i\} & \mbox{if $\mathcal{O}^{\oplus}_{s(v)}(i)$}
\\
\{s(v)\} & \mbox{otherwise}.
\end{cases}
\end{eqnarray}
\end{definition}

\subsubsection{Best-Neighbor}
Among social strategies, the straightforward greedy approach results in the {\em best-neighbor} strategy, which has been widely used in prior work \cite{lazer_network_2007, mason_propagation_2008, barkoczi_social_2016}. In this strategy, an agent simply compares the solutions of all agents in its neighborhood and adopts the best. While straightforward, this strategy makes a strong assumption about agent capabilities: access to the arbitrary comparisons.
\begin{definition}
The Best-Neighbor strategy $\mathcal{L}_{BN}(v)$ is defined as:
\begin{eqnarray}
\mathcal{L}_{BN}(v)
&\equiv&
\{ \, s \!\in\! S(v)
\mid
\forall \tilde{s} \!\in\! S(v) \, \
\lnot \, \mathcal{O}^{>}(\tilde{s}, s),
\end{eqnarray}
where $S(v)$ is the multiset of solutions of $v$'s neighbors.
\end{definition}

\subsubsection{Confident-Neighbor}
This paper introduces {\em confident-neighbor}, an alternative to the best-neighbor strategy which relies only on the single comparison oracle and which reduces to best-neighbor for interlocking pod networks. The confident neighbor strategy also represents a moderate level of social loafing \cite{karau_social_1993}, in which agents do not actively seek to improve their solution, but passively adopt better solutions if they are presented.

Confident neighbor proceeds in two stages. In the first stage, agents determine if their current solution is at least as good as all others in their neighborhood. If so, we call the agent {\em confident}. In the second stage, confident agents broadcast their solution to all of their neighbors. Non-confident agents choose randomly between any broadcast solutions they receive, or keep their original solution if they receive none.

\begin{definition}
The confident-neighbor strategy $\mathcal{L}_{CN}$ is defined as:
\begin{eqnarray}
C(v) &\equiv& \{
\, s(w)  \nonumber \\
&& \,
\mid
\forall w \!\in\! N(v) \, \forall u \!\in\! N(w) \,
\lnot \, \mathcal{O}^{>}_{s(w)}(s(u)) \,
\}
\\
\mathcal{L}_{\text{CN}}(v)
&\equiv& 
\begin{cases}
C(v) & \text{if } C(v) \neq \varnothing \\
\{ \, s(v) \, \}& \text{otherwise,}
\end{cases}
\end{eqnarray}
where $N(v)$ is the set of vertices neighboring $v$ and $s(v)$ is the current candidate solution for agent $v$.
\end{definition}

Confident-neighbor can produce a different candidate than best-neighbor as a result of two subtle but important considerations.
First, an agent's neighbors need not be adjacent to each other. Second, an agent's neighbors can be adjacent to others outside that agent's neighborhood. As a result, an agent might receive zero, one, or many broadcasts.
The exception is when agents belong to a clique (e.g., in network deliberation), in which case agents receive exactly one broadcast for each maximal solution in the clique, thus yielding the same results as best-neighbor.
This equivalence is notable because confident-neighbor requires only single-state
comparisons rather than the arbitrary state comparisons used by best-neighbor.
Best-neighbor thus requires stronger capabilities to achieve the same results under conventional deliberation,
while confident-neighbor requires the same set of capabilities under both conventional and network deliberation.


\subsubsection{Conform}
In some contexts, agents might rely on information other than solution quality in their learning strategies.
Agents might do so out of necessity if they are not able to compare arbitrary solutions.
Alternatively, agents might ignore solution quality due to social dynamics, e.g. social loafing \cite{karau_social_1993}. In these contexts, agents might instead evaluate solutions based on popularity, which produces the {\em conform} learning strategy.
When using the conform strategy, agents count the number of times each solution appears among their neighbors, and adopt the most popular.
\begin{definition}
The conform strategy $\mathcal{L}_{\text{C}}(v)$ is defined as:
\begin{eqnarray}
\mathcal{L}_{\text{C}}(v) &=&
\mode(S(v)),
\end{eqnarray}
where $S(v)$ is the multiset of candidate solutions for all vertices neighboring $v$, and $\mode()$ returns a set containing the mode or modes of a multiset.
\end{definition}
Note that the conform strategy does not depend on the objective function or any oracles, meaning it incorporates no new information about the quality of the solutions.

\subsubsection{Combining Learning Strategies}\label{subsubsec:combine}
Models of social learning often combine truly social strategies with individual learning \cite{lazer_network_2007,barkoczi_social_2016,gomez_clustering_2019}.
We find that the method used to combine social and individual strategies can result in a notable difference in outcome.
\begin{description}
\item[Parallel]{
Agents apply both social and individual learning strategies to their initial solution to produce two competing intermediate solutions, then adopt the better of the two. This method relies on the arbitrary comparison oracle.}
\item[Fallback]{
Agents first apply social learning to the current solution, but "fall back" to individual learning if the result is not an improvement on the original solution \cite{lazer_network_2007, barkoczi_social_2016,gomez_clustering_2019}. Fallback relies on the single comparison oracle and could be motivated by limited agent capability or social loafing.}
\item[Serial]{
Agents first perform individual learning to produce an intermediate solution, and then apply social learning to the intermediate solutions. This method does not rely on any of the solution comparison oracles.}
\end{description}
We follow existing literature by focusing on the fallback method.
As a robustness check, we also consider the parallel method,
which uses the same agent capabilities but is arguably more powerful.

Combined social learning strategies may also differ in their
{\em criticality} \cite{barkoczi_social_2016, rendell_rogersparadox_2010}.
After combining individual and social learning according to one of the above methods, non-critical agents immediately adopt the result.
Critical agents, on the other hand, compare the quality of the new solution to their previous solution, only adopting the new one if it provides an improvement.
Note that critical behavior relies on information about solution quality and requires single-solution comparison capability, or stronger.


\subsection{Statistical Methods}
All comparisons are made using two-tailed paired t-tests. Pairs of observations correspond to the same instance of the NK-model objective function.
Significance values have been corrected for multiple comparisons using the Bonferroni correction.

\section{Results}

Here we present the results of 1000 runs of an agent-based model of deliberation.
In each run, an NK model is generated (N=15, K=6) and the model is simulated for 300
iterations for each network/strategy combination.
The results of these simulations are shown in Figure \ref{fig:ch03results}.
Figures \ref{fig:results-frac-parallel} and \ref{fig:results-frac-fallback} show pairwise comparisons of solution quality between strategy/network settings. Figures \ref{fig:results-bn-parallel-dist}--\ref{fig:results-conf-fallback-dist} show the complete solution quality distributions for all settings.
Tables \ref{tab:t-instrat-fallback}--\ref{tab:t-innet-parallel} show the t-values and Bonferroni-corrected p-values for comparisons across networks and strategies.

We find that network deliberation identifies higher quality solutions than
conventional deliberation when agents use the conform strategy,
while requiring less time to converge. Within network deliberation,
we find that the structurally efficient random pod network outperforms the
structurally inefficient long-path network when agents use the conform strategy.
However, when either best-neighbor or confident-neighbor is used,
the inefficient network is preferable, consistent with findings for conventional
networks \cite{barkoczi_social_2016}. We also find that the confident-neighbor
strategy matches or outperforms best-neighbor across all networks,
despite using strictly less information about solution quality.


\begin{figure*}[]
    \centering
    \begin{minipage}{3.33in}
        \centering
        \includegraphics[width=3.33in]{chapters/figures/NetDelibABM/fig-perf-both.eps}
    \end{minipage}%
    \begin{minipage}{1.875in}
    \centering
    \includegraphics[width=1.875in]{chapters/figures/NetDelibABM/fig-converge-both.eps}
    \end{minipage}
	\caption{
	Solution quality and convergence time for agent-based simulations of different learning strategies. Error bars represent 95\% confidence interval. Results for network deliberation are shown shaded.
	}
    \label{fig:ch03results}
\end{figure*}

\begin{figure}
    \centering
    \includegraphics[width=3.33in]{chapters/figures/NetDelibABM/fig-result-frac-parallel.eps}
	\caption{Fraction of simulations in which the row condition outperforms the column condition. Results are shown for parallel individual learning. In the case of a tie, weight is divided evenly.}
    \label{fig:results-frac-parallel}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=3.33in]{chapters/figures/NetDelibABM/fig-result-frac-parallel.eps}
	\caption{Fraction of simulations in which the row condition outperforms the column condition. Results are shown for fallback individual learning. In the case of a tie, weight is divided evenly.}
    \label{fig:results-frac-fallback}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=3.33in]{chapters/figures/NetDelibABM/results-bn-parallel-dist.eps}
	\caption{Solution quality distribution for best-neighbor strategy with parallel individual learning.}
\end{figure}
    \label{fig:results-bn-parallel-dist}

\begin{figure}
    \centering
    \includegraphics[width=3.33in]{chapters/figures/NetDelibABM/results-cn-parallel-dist.eps}
	\caption{Solution quality distribution for confident-neighbor strategy with parallel individual learning.}
    \label{fig:results-cn-parallel-dist}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=3.33in]{chapters/figures/NetDelibABM/results-conf-parallel-dist.eps}
	\caption{Solution quality distribution for the conform strategy with parallel individual learning.}
    \label{fig:results-conf-parallel-dist}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=3.33in]{chapters/figures/NetDelibABM/results-bn-fallback-dist.eps}
	\caption{Solution quality distribution for best-neighbor strategy with fallback individual learning.}
    \label{fig:results-bn-fallback-dist}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=3.33in]{chapters/figures/NetDelibABM/results-cn-fallback-dist.eps}
	\caption{Solution quality distribution for confident-neighbor strategy with fallback individual learning.}
    \label{fig:results-cn-fallback-dist}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=3.33in]{chapters/figures/NetDelibABM/results-conf-fallback-dist.eps}
	\caption{Solution quality distribution for the conform strategy with fallback individual learning.}
    \label{fig:results-conf-fallback-dist}
\end{figure}

\begin{table*}[]
    \centering
    \begin{tabular}{l|ll|ccc}
        Strategy & Net A & Net B & $\overline{B - A}$ & t & p* \\
        \hline
Best Neighbor&Pref. Attach.&Small World&-8.70e-02&-1.33e+01&1.27e-35\\
Best Neighbor&Pref. Attach.&Long Path&-5.98e-02&-8.62e+00&1.51e-15\\
Best Neighbor&Pref. Attach.&Random Pod&-1.15e-02&-1.83e+00&4.01e+00\\
Best Neighbor&Small World&Pref. Attach.&8.70e-02&1.33e+01&1.27e-35\\
Best Neighbor&Small World&Long Path&2.72e-02&4.53e+00&3.88e-04\\
Best Neighbor&Small World&Random Pod&7.55e-02&1.18e+01&1.66e-28\\
Best Neighbor&Long Path&Pref. Attach.&5.98e-02&8.62e+00&1.51e-15\\
Best Neighbor&Long Path&Small World&-2.72e-02&-4.53e+00&3.88e-04\\
Best Neighbor&Long Path&Random Pod&4.83e-02&7.34e+00&2.69e-11\\
Best Neighbor&Random Pod&Pref. Attach.&1.15e-02&1.83e+00&4.01e+00\\
Best Neighbor&Random Pod&Small World&-7.55e-02&-1.18e+01&1.66e-28\\
Best Neighbor&Random Pod&Long Path&-4.83e-02&-7.34e+00&2.69e-11\\
\hline
Confident Neighbor&Pref. Attach.&Small World&-7.52e-02&-1.13e+01&2.94e-26\\
Confident Neighbor&Pref. Attach.&Long Path&2.39e-03&3.64e-01&4.30e+01\\
Confident Neighbor&Pref. Attach.&Random Pod&5.80e-02&8.88e+00&1.86e-16\\
Confident Neighbor&Small World&Pref. Attach.&7.52e-02&1.13e+01&2.94e-26\\
Confident Neighbor&Small World&Long Path&7.76e-02&1.21e+01&1.08e-29\\
Confident Neighbor&Small World&Random Pod&1.33e-01&1.92e+01&1.36e-68\\
Confident Neighbor&Long Path&Pref. Attach.&-2.39e-03&-3.64e-01&4.30e+01\\
Confident Neighbor&Long Path&Small World&-7.76e-02&-1.21e+01&1.08e-29\\
Confident Neighbor&Long Path&Random Pod&5.56e-02&8.50e+00&4.00e-15\\
Confident Neighbor&Random Pod&Pref. Attach.&-5.80e-02&-8.88e+00&1.86e-16\\
Confident Neighbor&Random Pod&Small World&-1.33e-01&-1.92e+01&1.36e-68\\
Confident Neighbor&Random Pod&Long Path&-5.56e-02&-8.50e+00&4.00e-15\\
\hline
Conform&Pref. Attach.&Small World&3.60e-02&6.78e+00&1.24e-09\\
Conform&Pref. Attach.&Long Path&-1.63e-01&-2.22e+01&2.25e-87\\
Conform&Pref. Attach.&Random Pod&-2.28e-01&-3.19e+01&9.08e-153\\
Conform&Small World&Pref. Attach.&-3.60e-02&-6.78e+00&1.24e-09\\
Conform&Small World&Long Path&-1.99e-01&-3.17e+01&1.59e-151\\
Conform&Small World&Random Pod&-2.64e-01&-4.43e+01&2.20e-236\\
Conform&Long Path&Pref. Attach.&1.63e-01&2.22e+01&2.25e-87\\
Conform&Long Path&Small World&1.99e-01&3.17e+01&1.59e-151\\
Conform&Long Path&Random Pod&-6.48e-02&-8.31e+00&1.79e-14\\
Conform&Random Pod&Pref. Attach.&2.28e-01&3.19e+01&9.08e-153\\
Conform&Random Pod&Small World&2.64e-01&4.43e+01&2.20e-236\\
Conform&Random Pod&Long Path&6.48e-02&8.31e+00&1.79e-14\\
\hline
    \end{tabular}
    \caption{Two-tailed paired t-values across networks, within strategies. Results are for fallback individual learning. For all rows, dof=999 and $p^*$ is the Bonferroni-corrected p-value with m=60.}
    \label{tab:t-instrat-fallback}
\end{table*}

\begin{table*}[]
    \centering
    \begin{tabular}{l|ll|ccc}
        Network & Strategy A & Strategy B & $\overline{B - A}$ & t & p* \\
        \hline
Pref. Attach.&Best Neighbor&Confident Neighbor&-6.43e-02&-9.39e+00&2.33e-18\\
Pref. Attach.&Best Neighbor&Conform&8.96e-02&1.19e+01&6.83e-29\\
Pref. Attach.&Confident Neighbor&Best Neighbor&6.43e-02&9.39e+00&2.33e-18\\
Pref. Attach.&Confident Neighbor&Conform&1.54e-01&2.06e+01&7.57e-77\\
Pref. Attach.&Conform&Best Neighbor&-8.96e-02&-1.19e+01&6.83e-29\\
Pref. Attach.&Conform&Confident Neighbor&-1.54e-01&-2.06e+01&7.57e-77\\
\hline
Small World&Best Neighbor&Confident Neighbor&-5.26e-02&-8.39e+00&9.70e-15\\
Small World&Best Neighbor&Conform&2.13e-01&3.52e+01&4.46e-175\\
Small World&Confident Neighbor&Best Neighbor&5.26e-02&8.39e+00&9.70e-15\\
Small World&Confident Neighbor&Conform&2.65e-01&4.45e+01&1.24e-237\\
Small World&Conform&Best Neighbor&-2.13e-01&-3.52e+01&4.46e-175\\
Small World&Conform&Confident Neighbor&-2.65e-01&-4.45e+01&1.24e-237\\
\hline
Long Path&Best Neighbor&Confident Neighbor&-2.12e-03&-3.42e-01&4.39e+01\\
Long Path&Best Neighbor&Conform&-1.37e-02&-1.71e+00&5.20e+00\\
Long Path&Confident Neighbor&Best Neighbor&2.12e-03&3.42e-01&4.39e+01\\
Long Path&Confident Neighbor&Conform&-1.16e-02&-1.44e+00&8.96e+00\\
Long Path&Conform&Best Neighbor&1.37e-02&1.71e+00&5.20e+00\\
Long Path&Conform&Confident Neighbor&1.16e-02&1.44e+00&8.96e+00\\
\hline
Random Pod&Best Neighbor&Confident Neighbor&5.18e-03&8.40e-01&2.41e+01\\
Random Pod&Best Neighbor&Conform&-1.27e-01&-1.64e+01&1.07e-51\\
Random Pod&Confident Neighbor&Best Neighbor&-5.18e-03&-8.40e-01&2.41e+01\\
Random Pod&Confident Neighbor&Conform&-1.32e-01&-1.67e+01&1.39e-53\\
Random Pod&Conform&Best Neighbor&1.27e-01&1.64e+01&1.07e-51\\
Random Pod&Conform&Confident Neighbor&1.32e-01&1.67e+01&1.39e-53\\
\hline
    \end{tabular}
    \caption{Two-tailed paired t-values across strategies, within networks. Results are for fallback individual learning. For all rows, dof=999 and $p^*$ is the Bonferroni-corrected p-value with m=60.}
    \label{tab:t-innet-fallback}
\end{table*}

\begin{table*}[]
    \centering
    \begin{tabular}{l|ll|ccc}
        Strategy & Net A & Net B & $\overline{B - A}$ & t & p* \\
    \hline
Best Neighbor&Pref. Attach.&Small World&-6.75e-02&-1.13e+01&2.48e-26\\
Best Neighbor&Pref. Attach.&Long Path&-4.24e-02&-6.78e+00&1.25e-09\\
Best Neighbor&Pref. Attach.&Random Pod&-8.14e-03&-1.32e+00&1.12e+01\\
Best Neighbor&Small World&Pref. Attach.&6.75e-02&1.13e+01&2.48e-26\\
Best Neighbor&Small World&Long Path&2.50e-02&4.25e+00&1.42e-03\\
Best Neighbor&Small World&Random Pod&5.93e-02&9.38e+00&2.54e-18\\
Best Neighbor&Long Path&Pref. Attach.&4.24e-02&6.78e+00&1.25e-09\\
Best Neighbor&Long Path&Small World&-2.50e-02&-4.25e+00&1.42e-03\\
Best Neighbor&Long Path&Random Pod&3.43e-02&5.36e+00&6.14e-06\\
Best Neighbor&Random Pod&Pref. Attach.&8.14e-03&1.32e+00&1.12e+01\\
Best Neighbor&Random Pod&Small World&-5.93e-02&-9.38e+00&2.54e-18\\
Best Neighbor&Random Pod&Long Path&-3.43e-02&-5.36e+00&6.14e-06\\
\hline
Confident Neighbor&Pref. Attach.&Small World&-4.49e-02&-7.20e+00&7.03e-11\\
Confident Neighbor&Pref. Attach.&Long Path&-5.03e-03&-7.65e-01&2.67e+01\\
Confident Neighbor&Pref. Attach.&Random Pod&2.61e-02&4.02e+00&3.74e-03\\
Confident Neighbor&Small World&Pref. Attach.&4.49e-02&7.20e+00&7.03e-11\\
Confident Neighbor&Small World&Long Path&3.99e-02&6.96e+00&3.80e-10\\
Confident Neighbor&Small World&Random Pod&7.10e-02&1.18e+01&1.98e-28\\
Confident Neighbor&Long Path&Pref. Attach.&5.03e-03&7.65e-01&2.67e+01\\
Confident Neighbor&Long Path&Small World&-3.99e-02&-6.96e+00&3.80e-10\\
Confident Neighbor&Long Path&Random Pod&3.11e-02&4.89e+00&7.03e-05\\
Confident Neighbor&Random Pod&Pref. Attach.&-2.61e-02&-4.02e+00&3.74e-03\\
Confident Neighbor&Random Pod&Small World&-7.10e-02&-1.18e+01&1.98e-28\\
Confident Neighbor&Random Pod&Long Path&-3.11e-02&-4.89e+00&7.03e-05\\
\hline
Conform&Pref. Attach.&Small World&3.67e-02&6.85e+00&7.73e-10\\
Conform&Pref. Attach.&Long Path&-1.73e-01&-2.40e+01&7.10e-99\\
Conform&Pref. Attach.&Random Pod&-2.77e-01&-4.08e+01&6.29e-213\\
Conform&Small World&Pref. Attach.&-3.67e-02&-6.85e+00&7.73e-10\\
Conform&Small World&Long Path&-2.09e-01&-3.46e+01&3.46e-171\\
Conform&Small World&Random Pod&-3.14e-01&-5.68e+01&1.63e-313\\
Conform&Long Path&Pref. Attach.&1.73e-01&2.40e+01&7.10e-99\\
Conform&Long Path&Small World&2.09e-01&3.46e+01&3.46e-171\\
Conform&Long Path&Random Pod&-1.04e-01&-1.43e+01&8.55e-41\\
Conform&Random Pod&Pref. Attach.&2.77e-01&4.08e+01&6.29e-213\\
Conform&Random Pod&Small World&3.14e-01&5.68e+01&1.63e-313\\
Conform&Random Pod&Long Path&1.04e-01&1.43e+01&8.55e-41\\
    \hline
    \end{tabular}
    \caption{Two-tailed paired t-values across networks, within strategies. Results are for parallel individual learning. For all rows, dof=999 and $p^*$ is the Bonferroni-corrected p-value with m=60.}
    \label{tab:t-instrat-parallel}
\end{table*}

\begin{table*}[]
    \centering
    \begin{tabular}{l|ll|ccc}
        Network & Strategy A & Strategy B & $\overline{B - A}$ & t & p* \\
    \hline
Pref. Attach.&Best Neighbor&Confident Neighbor&-3.78e-02&-5.94e+00&2.39e-07\\
Pref. Attach.&Best Neighbor&Conform&2.00e-01&2.77e+01&3.28e-124\\
Pref. Attach.&Confident Neighbor&Best Neighbor&3.78e-02&5.94e+00&2.39e-07\\
Pref. Attach.&Confident Neighbor&Conform&2.38e-01&3.40e+01&1.67e-167\\
Pref. Attach.&Conform&Best Neighbor&-2.00e-01&-2.77e+01&3.28e-124\\
Pref. Attach.&Conform&Confident Neighbor&-2.38e-01&-3.40e+01&1.67e-167\\
\hline
Small World&Best Neighbor&Confident Neighbor&-1.52e-02&-2.74e+00&3.80e-01\\
Small World&Best Neighbor&Conform&3.05e-01&5.30e+01&1.25e-290\\
Small World&Confident Neighbor&Best Neighbor&1.52e-02&2.74e+00&3.80e-01\\
Small World&Confident Neighbor&Conform&3.20e-01&5.89e+01&0.00e+00\\
Small World&Conform&Best Neighbor&-3.05e-01&-5.30e+01&1.25e-290\\
Small World&Conform&Confident Neighbor&-3.20e-01&-5.89e+01&0.00e+00\\
\hline
Long Path&Best Neighbor&Confident Neighbor&-3.88e-04&-6.64e-02&5.68e+01\\
Long Path&Best Neighbor&Conform&7.02e-02&9.34e+00&3.64e-18\\
Long Path&Confident Neighbor&Best Neighbor&3.88e-04&6.64e-02&5.68e+01\\
Long Path&Confident Neighbor&Conform&7.06e-02&9.15e+00&1.83e-17\\
Long Path&Conform&Best Neighbor&-7.02e-02&-9.34e+00&3.64e-18\\
Long Path&Conform&Confident Neighbor&-7.06e-02&-9.15e+00&1.83e-17\\
\hline
Random Pod&Best Neighbor&Confident Neighbor&-3.59e-03&-5.46e-01&3.51e+01\\
Random Pod&Best Neighbor&Conform&-6.84e-02&-9.54e+00&6.11e-19\\
Random Pod&Confident Neighbor&Best Neighbor&3.59e-03&5.46e-01&3.51e+01\\
Random Pod&Confident Neighbor&Conform&-6.48e-02&-8.96e+00&9.43e-17\\
Random Pod&Conform&Best Neighbor&6.84e-02&9.54e+00&6.11e-19\\
Random Pod&Conform&Confident Neighbor&6.48e-02&8.96e+00&9.43e-17\\
    \hline
    \end{tabular}
    \caption{Two-tailed paired t-values across strategies, within networks. Results are for parallel individual learning. For all rows, dof=999 and $p^*$ is the Bonferroni-corrected p-value with m=60.}
    \label{tab:t-innet-parallel}
\end{table*}


\subsection{Network vs. Conventional Deliberation}
We are particularly interested in comparing the performance of network deliberation to that of conventional single-group deliberation.

When the conform strategy is used, we find that both network deliberation conditions yield better performance than conventional networks (Figure \ref{fig:results-conform-diff}).  The effect size is substantial. The solution quality distributions (Figure \ref{fig:results-conform-dist}) show that network deliberation finds maximal solutions more frequently than conventional. Furthermore, non-maximal solutions found by network deliberation are typically higher-quality than those found by conventional.
The quality difference is due to a reduction in solution quality in conventional networks that is not seen for network deliberation.
Agents' transitions between basins of attraction provide additional insight into the mechanism.
In conventional deliberation, transitions to higher and lower quality basins are nearly equally common (Figure \ref{fig:conform-transition-basin}).
For network deliberation, there is a small bias towards higher quality basins, particularly as time progresses.
Similarly, network deliberation typically converges to a single basin, while multiple basins remain occupied for conventional deliberation (Figure \ref{fig:conform-basin}).
So we see that in the conventional setting, agents converge quickly to sub-optimal basins, and become stuck.
While in network deliberation, agents converge quickly but continue to explore and transition to preferable basins.
Why does this behavior occur for the conform strategy? And how does network deliberation provide protection?
Under the conform strategy, popular solutions can spread quickly within highly-clustered communities, even if those solutions are low-quality,
resulting in an information cascade \cite{banerjee_simple_1992}.
Furthermore, once a community is saturated with a single solution, agents mutually reinforce each other, preventing any from exploring alternative solutions.
However, network deliberation provides cross-cutting connections to multiple pods which can outweigh an information cascade originating in a single pod.
Network deliberation thus protects against detrimental effects of social influence, such as information cascades.

In many problem-solving settings, improvements to quality can come at the cost of speed, as more time is devoted to exploring novel solutions.
Such a reduction in speed can result in increased latency and reduced productivity.
However, we find that network deliberation improves both performance and speed when the conform strategy is used, suggesting that interlocking network structure enables more efficient exploration.

In contrast to the conform strategy, network deliberation performs comparably or slightly worse than conventional under the best-neighbor and confident-neighbor strategies. This finding suggests that network deliberation is most beneficial when individuals are relying on social influence rather thank solution quality, but can potentially be applied in either context.

\begin{figure}
    \centering
    \includegraphics[width=3in]{chapters/figures/NetDelibABM/fig-results-netdelib-diff.eps}
	\caption{Difference in solution quality between network deliberation and conventional deliberation. Plots for Best/Confident-Neighbor show results for long-path and small-world networks, while the plot for conform shows results for random-pod and preferential-attachment networks. Results are shown for parallel individual learning.}
    \label{fig:results-conform-diff}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=3in]{chapters/figures/NetDelibABM/fig-results-conform-dist.eps}
	\caption{Distributions of solution quality for social learning on a preferential-attachment network and random-pod network deliberation. Results are shown for parallel individual learning.}
    \label{fig:results-conform-dist}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=6in]{chapters/figures/NetDelibABM/fig-conform-transition-basin.eps}
	\caption{Number of agent transitions to basins of attraction with higher/lower maxima at each time step.}
    \label{fig:conform-transition-basin}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=3in]{chapters/figures/NetDelibABM/fig-conform-basin.eps}
	\caption{Number of basins of attraction occupied by at least one agent at each time step.}
    \label{fig:conform-basin}
\end{figure}


The conform strategy models contexts in which individuals rely heavily on social influence. Such contexts can arise from pro-conformity social norms, high levels of trust, social loafing, and limited information. As a form of rational ignorance, conformist behavior can conserve individual resources, but poses the risk of forming information cascades and propagating misinformation and inferior or harmful innovations \cite{banerjee_simple_1992}. Our results show that network deliberation can greatly improve the outcome of deliberation in the presence of strong social influence. This insight suggests one possible mechanism behind the success of mass deliberative projects, as well as the potential for network-based interventions in mass deliberation.

\subsection{Structural Efficiency}

The speed at which information can move through a communication network, i.e., it's structural efficiency, has been found to influence the success of collective problem-solving. We might expect the same to be true in network deliberation. In network deliberation, network structure is created through group membership. We compare the results of two group assignment methods: a random pod assignment yielding an efficient network, and a long-path method yielding a less efficient network (Figure \ref{fig:results-netdelib-efficiency}). We find that for greedy strategies (best-neighbor and confident-neighbor), the inefficient network yields the better results. The results are opposite for the conform strategy, with the efficient network yielding better results. These results are consistent with those observed for conventional single-group networks \cite{barkoczi_social_2016}, but to the best of our knowledge, they have not been explored for interlocking network like the ones we use to model network deliberation.

\begin{figure}
    \centering
    \includegraphics[width=3.33in]{chapters/figures/NetDelibABM/fig-results-netdelib-efficiency.eps}
	\caption{Difference in solution quality between random-pod (efficient) and long-path (inefficient) network. Results are shown for parallel individual learning.}
    \label{fig:results-netdelib-efficiency}
\end{figure}

The role of structural efficiency in collective tasks can be partially attributed to diversity \cite{lazer_network_2007, hong_groups_2004}. Greedy strategies such as best/confident-neighbor result in agents adopting the highest quality solution they have seen so far and discarding all others. Such strategies quickly reduce the number of solutions present in the population, quickly identifying a local maximum at the cost of diversity. As a result, agents duplicate efforts during individual learning, exploring mutations of the same solution. Inefficient networks can mitigate this effect by slowing the spread of high-quality solutions and allowing time for the exploration of new and potentially better solutions. Our results show exactly this, with long-path pod assignment improving the performance of greedy algorithms. The improved performance of the conform strategy in random pod assignment can be attributed to the converse of the above. When social influence is strong, efficient networks can interrupt information cascades by introducing new information from far across the network.
Our results are thus generally consistent with existing theory on structural efficiency and collective problem-solving.
However, we note an interesting deviation: under the conform strategy, long-path network deliberation out-performs the two conventional networks, both of which are more efficient.

\subsection{Confident-Neighbor Strategy}

In general, the best-neighbor strategy requires strong assumptions about agents' ability to evaluate solutions. In the special case of network deliberation, the assumptions can be weakened by applying a variant of the strategy, which has not been previously studied. The variant, which we call {\em confident-neighbor}, is equivalent to best-neighbor for network deliberation, but not in general. Confident-neighbor thus provides a comparison between network deliberation and conventional single-group deliberation under weaker informational assumptions than best-neighbor. Surprisingly, the weaker assumptions of confident-neighbor lead to higher performance on the preferential attachment network, and similar performance on the-small world network (within our statistical resolution). Figure \ref{fig:results-confident} shows histograms of the quality difference between confident-neighbor and best-neighbor for both conventional networks. In the majority of cases, both strategies find comparable solutions. However, we find that when these strategies do find different solutions, those found by confident-neighbor are more often better, at least for the preferential-attachment network.

\begin{figure}
    \centering
    \includegraphics[width=3.33in]{chapters/figures/NetDelibABM/fig-results-confident.eps}
	\caption{Histograms of the difference in solution quality between confident-neighbor and best-neighbor for both conventional networks.
	Results are shown for parallel individual learning.}
    \label{fig:results-confident}
\end{figure}

As with conform, the confident-neighbor strategy can be used when available information is insufficient to use best-neighbor. Unlike conform, confident-neighbor models weak social influence. The improved performance of confident-neighbor over best-neighbor, despite using less information, can be attributed at least in part to diversity and exploration. Confident neighbor takes significantly longer to converge, suggesting it maintains a diversity of solutions and allows more time to explore new solutions. Our results suggest several interventions with the potential to improve conventional single-group deliberation. In the presence of weak social influence, reducing the amount of information available can counter-intuitively increase performance. Conversely, when information is already limited, performance might be increased by weakening social influence. However, while confident-neighbor can increase the success of conventional deliberation, it performs comparably to network deliberation using the conform strategy, suggesting that when social influence is strong, network deliberation remains preferable.

\section{Discussion}

We have proposed and evaluated a model of mass deliberation based on the observed interlocking network structure of successful mass-deliberative projects.
Our central finding suggests that network deliberation can improve deliberative output in the presence of strong social influence (i.e., the conform strategy).
Individuals rely on social influence for a number of reasons.
Strong social influence can stem from social factors such as trust or social loafing.
Alternatively, social influence can be used to supplement individual skill when it is insufficient for a given task.
We see that a variety of contexts can lead to reliance on social influence.
Empirical evidence suggests the presence of strong social influence in real-world collaborations such as Wikipedia \cite{platt_network_2018} and in lab studies of human collaboration \cite{mason_collaborative_2012, barkoczi_social_2016}.
In the case of weak social influence (i.e., the  best-neighbor and confident-neighbor strategies), we do not observe a substantial difference in output quality between network deliberation and conventional single-group deliberation. In all cases, network deliberation performs better than single-group deliberation or comparatively well. 
Our results suggest that network deliberation is both a contributing mechanism to the success of mass-deliberative projects as well as a useful tool for the design of large-scale sociotechnical systems and interventions for existing systems.

How does network deliberation improve collaborative output?
We have found that network deliberation protects against the negative effects of strong social influence by disrupting information cascades.
Rather than allowing individuals to converge to polarized communities with suboptimal solutions, the interlocking-pod structure of network deliberation allows agents to continue to explore and improve their solutions.

In order to distinguish between the effects of social influence and individual ability, we have introduced the confident-neighbor strategy, which (unlike best-neighbor) assumes the same agent capabilities as the conform strategy (in a critical learning setting).
While not central to our study of network deliberation, our findings regarding the 
confident-neighbor strategy have important implications for the study of social learning in general. 
Our findings show that just as reducing network efficiency can improve solution quality by raising diversity, reducing the available information can improve the performance of greedy learning strategies. Our investigation of confident-neighbor stems from a careful formal analysis of the information required by each strategy. The surprisingly high performance of confident-neighbor shows the importance of isolating social influence from agent ability.

Our work is limited by several assumptions, which require further investigation to achieve greater generalizability. We assume a static objective function, while real-world objectives can change with changing external conditions as well as changing individual preferences. We also assume a homogeneous strategy across all agents, which raises questions of how different strategies interact within the same collaboration. We have also assumed that agents are truthful and do not form coalitions outside of the existing network, both assumptions which can be violated in real-world settings.
We have also assumed that agents have access to the full solution string, and that agents incorporate all bits of the string into their quality estimates. Extremely complex real-world tasks might be better-modeled by a collection of sub-tasks, with agents having access to only a subset of the solution string.
Finally, we have limited our study to learning strategies that explore new solutions solely through small mutations of existing solutions. Other potential strategies might generate entirely new solutions, for example, by combining parts of multiple previous solutions. Such generative strategies would add the ability to explore a wider range of solutions. While our work extends the understanding of social learning and collaboration, many questions still remain open for exploration.

Effective mass deliberation is key to realizing the potential of large-scale collective action and the democratizing potential of the internet. Our findings suggest that the presence of small and interlocking pods can contribute to the success of mass deliberation, particularly when the topic is complex and social influence is strong. These findings have implications for a range of deliberative contexts: participatory government and budgeting, worker-owned cooperatives, grassroots social movements, and the governance of decentralized systems (e.g., cryptocurrency). By enabling better deliberation at larger scales, we hope this work will contribute to democratizing the governance of large sociotechnical systems, and empowering the individuals impacted by those systems.

