% ACM
%\documentclass[sigconf]{acmart}

% Manuscript
\documentclass[10pt,twocolumn]{article}
\usepackage{natbib}

\usepackage{booktabs} % For formal tables

% Document-specific packages
\usepackage{epigraph}

% Document-specific definitions
\newcommand{\beq}{\begin{eqnarray}}
\newcommand{\eeq}{\end{eqnarray}}

% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
%\acmDOI{10.475/123_4}

% ISBN
%\acmISBN{123-4567-24-567/08/06}

%Conference
%\acmConference[WOODSTOCK'97]{ACM Woodstock conference}{July 1997}{El
%  Paso, Texas USA} 
%\acmYear{1997}
%\copyrightyear{2016}

%\acmPrice{15.00}

\begin{document}
\title{Network Structure, Efficiency, and Performance in WikiProjects}

% ACM
%\author{Edward L. Platt}
%\affiliation{%
%  \institution{University of Michigan}
%  \city{Ann Arbor} 
%  \state{Michigan} 
%}
%\email{elplatt@umich.edu}

% Manuscript
\author{Edward L. Platt \\ University of Michigan \\ Ann Arbor, Michigan \\ elplatt@umich.edu}

% The default list of authors is too long for headers}
%\renewcommand{\shortauthors}{B. Trovato et al.}


% ACM
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
%\begin{CCSXML}
%\end{CCSXML}

% ACM
%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}
%\ccsdesc{Computer systems organization~Robotics}
%\ccsdesc[100]{Networks~Network reliability}


% ACM
% \keywords{collaboration, peer production, wikipedia}


\maketitle

\begin{abstract}
TODO
\end{abstract}

\section{Introduction}
\epigraph
{The problem with Wikipedia is that it only works in practice. In theory, it can never work.}
{Miikka Ryokas \cite{cohen_latest_2007} }

Wikipedia successful decentralized.

Efficiency Performance

Coeditor network

NK Simulations

Contributions

\section{Background and Related Work}

TODO

\section{Empirical Methods}

\subsection{Data}

Our analysis combines multiple datasets from the English-language Wikipedia.
For information about edit history, we used a publicly-available dataset containing
metadata about all edits between TODO.
To get the rating history of each article,
we wrote a script to scrape the daily logs produced by WPBot1.0 for each WikiProject
between TODO.
Finally, we used a publicly-available log of page events (including rename events)
to reconstruct the unique identifier for each article title mentioned in the rating history logs.

\subsection{Efficiency and Performance}

To model the relationship between performance, efficiency, and network structure,
we must have a way to quantify performance and efficiency.
We define these quantities on a per-WikiProject basis to enable comparisons across different
projects.
Our definitions are inpsired by work modeling collective problem-solving as an optimization
problem \cite{}.
The effieincy quantifies how quikly a solution is reached,
while the performance quantifies how good the eventual solution is.

For a WikiProject, efficiency quantifies how quickly project participants can improve the
assessed quality of an article.
Quality assessments are made through consensus of the project participants themselves,
so different projects can have different standards and practices for assessing article quality.
So the efficiency is not a measure of how quickly some objective measure of quality improves,
but rather of how quickly the project participants can reach consensus on the improvments that
need to be made and make those improvements.
Because our definition relies on assessment transitions, we define efficiency variables for
each of the project-level quality assessments: A, B, and C.
If $T(W,G)$ is the set of transitions in project $W$ from below grade $G$ to grade $G$ (or higher),
then we quantify the efficiency $E(W,G)$ as:
\beq
E(W,G) &=& \sum_{t \in W(P,G)} \left[ \frac{r(t)}{g(t)} \right]^{-1},
\eeq
where $r(t)$ is the number of revisions since the previous grade transition,
and $g(t)$ is the number of grade levels crossed by transition $t$.
The $g(t)$ term is added because an assessments often raise article quality by several
grades, in which case the revisions are divided evenly between all grade levels achieved.
It is also worth noting that we measure efficiency in terms of revisions made,
rather than time passed.
We focus on revisions because the amount of work done on an article varies widely from day to day.

For performance, we wish to quantify how good articles tend to be when they reach a stable state.
Measuring performance is difficult for several reasons:
there is no objective meaasure of article quality avaialbe,
and articles are always changing, making it difficult to know which articles should be considered
complete or stable.
We use an extremely simple performance measure that gives surprisingly consistent results.
In addition to per-project quality assessment, articles can be given ``featured article'' or
``good article'' status.
The criteria for these statuses are consistent across all of Wikipedia,
and any editor can participate in the discussion and decision to award good or featured
status.
In other words, the good and featured statuses are more objective than per-project assessments.
Our performance measure $P(W)$ is just the percentage of articles in project $W$ which have reached
good or featured status:
\beq
P(W) &=& \frac{f(W) + g(W)}{n(W)},
\eeq
where $f(W)$ and $g(W)$ are the numbered of featured and good articles respectively,
and $n(W)$ is the total number of articles.

\subsection{Coeditor Networks}

For each WikiProject, we compare the efficiency and performance measures to the structural
properties of its coeditor network.
The {\em coeditor network} of a WikiProject consists of nodes representing editors.
Two editors are connected when they have both edited the same article or talk page.
The edges are directed, with the direction representing the direction of
{\em plausible information flow};
an edge from editor A to editor B exists if A edited an article and then B edited the same article at
a later time.
Edges can exist in both directions e.g., if an article was edited first by A, then by B, and again by A.
For simplicity, we assign all edges unit weight.
We focus on three structural properties: degree, characteristic path length, and min-cut.

The node degree distribution is the simplest structural property we analyze for WikiProject
coeditor networks.
The in-degree (out-degree) of a node is the number of edges to (from) that node.
Taking the average of either in-degree or out-degree gives the same value:
the {\em mean degree} of the network.
In our context, the mean degree represents how many others a given editor has collaborated with.
We also consider the {\em skewness} of the in-degree and out-degree distributions.
A large positive degree skewness value for a WikiProject coeditor network
implies that a small number of editors have a very large number of collaborators,
while a small positive value implies that the editors having the most collaborators
don't have many more than a typical editor.

We also calculate the characteristic path length for each WikiProject coeditor network.
The {\em distance} from editor A to editor B is the length of the shortest path from A to B.
The {\em characteristic path length} is the mean distance between all editor pairs.
If no path exists between two editors, we exclude that pair from the mean.
For brevity, we will simply refer to this quantity as the {\em path length}.
The path length represents how quickly information can move through the network.
Networks with longer paths require more interactions for information to propagate through
the network.

Our final network measure quantifies the connectivy of a project's coeditor network using
min-cut size.
The minimum $st$-cut between nodes $s$ and $t$ is the set of edges that must be removed in order that
no path exists from $s$ to $t$.
The minimum cut (min-cut) of a graph is the smallest minimum $st$-cut over all node pairs $st$. 
The size of the graph min-cut quantifies the connectivity of a graph,
but only incorporates information about edges lying on paths crossing the min-cut.
Instead, we use the mean size of all minimum $st$-cuts, which we refer to as the
{\em mean min-cut}.
This measure quantifies the number of redundant paths information can take through the network.
Networks with higher redundancy are more resilient to errors on one path \cite{}
and allow innovation to propagate through complex contagion,
in which innovations are only adopted after multiple exposures thorugh different sources
\cite{}.

\subsection{Model}

OLS, controls

\subsection{Empirical Results}

TODO

\section{Numerical Simulation}

Intro

\subsection{Learning Strategies}

Individual learning

Social learning and iteration

Best neighbor

Conformity

Consensus

\subsection{Network family}

TODO

Base network

Duplication and rewiring

\subsection{Simulation results}

TODO

\section{Discussion}
TODO

\section{Conclusion}
TODO

\section{Acknowledgements}
Daniel Romero.
Danielle Livneh, Karthik Ramanathan.
Yan Chen, Tanya Rosenblat.
MIT Center for Civic Media.
Cooperation Working Group at the Harvard Berkman-Klein Center.
School of Information.

% ACM
%\bibliographystyle{ACM-Reference-Format}

% Manuscript
\bibliographystyle{acm}

\bibliography{paper} 

\end{document}
