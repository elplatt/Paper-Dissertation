% ACM
%\documentclass[sigconf]{acmart}

% Manuscript
\documentclass[10pt,twocolumn]{article}
\usepackage[numbers]{natbib}

\usepackage{booktabs} % For formal tables

% Document-specific packages
\usepackage{epigraph}
\usepackage[flushleft]{threeparttable}

% Document-specific definitions
\newcommand{\beq}{\begin{eqnarray}}
\newcommand{\eeq}{\end{eqnarray}}
\newcommand{\+}{\phantom{-}}

% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
%\acmDOI{10.475/123_4}

% ISBN
%\acmISBN{123-4567-24-567/08/06}

%Conference
%\acmConference[WOODSTOCK'97]{ACM Woodstock conference}{July 1997}{El
%  Paso, Texas USA} 
%\acmYear{1997}
%\copyrightyear{2016}

%\acmPrice{15.00}

\begin{document}
\title{Network Structure, Efficiency, and Performance in WikiProjects}

% ACM
%\author{Edward L. Platt}
%\affiliation{%
%  \institution{University of Michigan}
%  \city{Ann Arbor} 
%  \state{Michigan} 
%}
%\email{elplatt@umich.edu}

% Manuscript
\author{Edward L. Platt \\ University of Michigan \\ Ann Arbor, Michigan \\ elplatt@umich.edu}

% The default list of authors is too long for headers}
%\renewcommand{\shortauthors}{B. Trovato et al.}


% ACM
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
%\begin{CCSXML}
%\end{CCSXML}

% ACM
%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}
%\ccsdesc{Computer systems organization~Robotics}
%\ccsdesc[100]{Networks~Network reliability}


% ACM
% \keywords{collaboration, peer production, wikipedia}


\maketitle

\begin{abstract}
TODO
\end{abstract}

\section{Introduction}
\epigraph
{The problem with Wikipedia is that it only works in practice. In theory, it can never work.}
{Miikka Ryokas \cite{cohen_latest_2007} }

Wikipedia successful decentralized.

Efficiency Performance

Coeditor network

NK Simulations

Contributions

\section{Background and Related Work}

TODO

\section{Empirical Methods}

\subsection{Data}

Our analysis combines multiple datasets from the English-language Wikipedia.
For information about edit history, we used a publicly-available dataset containing
metadata about all edits between TODO.
To get the rating history of each article,
we wrote a script to scrape the daily logs produced by WP 1.0 Bot for each WikiProject
between TODO.
Finally, we used a publicly-available log of page events (including rename events)
to reconstruct the unique identifier for each article title mentioned in the rating history logs.

\subsection{Efficiency and Performance}

To model the relationship between performance, efficiency, and network structure,
we must have a way to quantify performance and efficiency.
We define these quantities on a per-WikiProject basis to enable comparisons across different
projects.
Our definitions are inpsired by work modeling collective problem-solving as an optimization
problem \cite{lazer_network_2007, mason_propagation_2008, mason_collaborative_2012, grim_scientific_2013, barkoczi_social_2016}.
The efficiency quantifies how quikly a solution is reached,
while the performance quantifies how good the eventual solution is.

For a WikiProject, efficiency quantifies how quickly project participants can improve the
assessed quality of an article.
Quality assessments are made through consensus of the project participants themselves,
so different projects can have different standards and practices for assessing article quality.
So the efficiency is not a measure of how quickly some objective measure of quality improves,
but rather of how quickly the project participants can reach consensus on the improvments that
need to be made and make those improvements.
Because our definition relies on assessment transitions, we define efficiency variables for
each of the project-level quality assessments: A, B, and C.
If $T(W,G)$ is the set of transitions in project $W$ from below grade $G$ to grade $G$ (or higher),
then we quantify the efficiency $E(W,G)$ as:
\beq
E(W,G) &=& \sum_{t \in W(P,G)} \left[ \frac{r(t)}{g(t)} \right]^{-1},
\eeq
where $r(t)$ is the number of revisions since the previous grade transition,
and $g(t)$ is the number of grade levels crossed by transition $t$.
The $g(t)$ term is added because an assessments often raise article quality by several
grades, in which case the revisions are divided evenly between all grade levels achieved.
It is also worth noting that we measure efficiency in terms of revisions made,
rather than time passed.
We focus on revisions because the amount of work done on an article varies widely from day to day.

For performance, we wish to quantify how good articles tend to be when they reach a stable state.
Measuring performance is difficult for several reasons:
there is no objective meaasure of article quality avaialbe,
and articles are always changing, making it difficult to know which articles should be considered
complete or stable.
We use an extremely simple performance measure that gives surprisingly consistent results.
In addition to per-project quality assessment, articles can be given ``featured article'' or
``good article'' status.
The criteria for these statuses are consistent across all of Wikipedia,
and any editor can participate in the discussion and decision to award good or featured
status.
In other words, the good and featured statuses are more objective than per-project assessments.
Our performance measure $P(W)$ is just the percentage of articles in project $W$ which have reached
good or featured status:
\beq
P(W) &=& \frac{f(W) + g(W)}{n(W)},
\eeq
where $f(W)$ and $g(W)$ are the numbered of featured and good articles respectively,
and $n(W)$ is the total number of articles.

\subsection{Coeditor Networks}

For each WikiProject, we compare the efficiency and performance measures to the structural
properties of its coeditor network.
The {\em coeditor network} of a WikiProject consists of nodes representing editors.
Two editors are connected when they have both edited the same article or talk page.
The edges are directed, with the direction representing the direction of
{\em plausible information flow};
an edge from editor A to editor B exists if A edited an article and then B edited the same article at
a later time.
Edges can exist in both directions e.g., if an article was edited first by A, then by B, and again by A.
For simplicity, we assign all edges unit weight.
We focus on three structural properties: degree, characteristic path length, and min-cut.

The node degree distribution is the simplest structural property we analyze for WikiProject
coeditor networks.
The in-degree (out-degree) of a node is the number of edges to (from) that node.
Taking the average of either in-degree or out-degree gives the same value:
the {\em mean degree} of the network.
In our context, the mean degree represents how many others a given editor has collaborated with.
We also consider the {\em skewness} of the in-degree and out-degree distributions.
A large positive degree skewness value for a WikiProject coeditor network
implies that a small number of editors have a very large number of collaborators,
while a small positive value implies that the editors having the most collaborators
don't have many more than a typical editor.

We also calculate the characteristic path length for each WikiProject coeditor network.
The {\em distance} from editor A to editor B is the length of the shortest path from A to B.
The {\em characteristic path length} is the mean distance between all editor pairs.
If no path exists between two editors, we exclude that pair from the mean.
For brevity, we will simply refer to this quantity as the {\em path length}.
The path length represents how quickly information can move through the network.
Networks with longer paths require more interactions for information to propagate through
the network,
which has been shown to reduce efficiency in some settings
\cite{mason_propagation_2008,barkoczi_social_2016}.

Our final network measure quantifies the connectivy of a project's coeditor network using
min-cut size.
The minimum $st$-cut between nodes $s$ and $t$ is the set of edges that must be removed in order that
no path exists from $s$ to $t$.
The minimum cut (min-cut) of a graph is the smallest minimum $st$-cut over all node pairs $st$. 
The size of the graph min-cut quantifies the connectivity of a graph,
but only incorporates information about edges lying on paths crossing the min-cut.
Instead, we use the mean size of all minimum $st$-cuts, which we refer to as the
{\em mean min-cut}.
This measure quantifies the number of redundant paths information can take through the network.
Networks with higher redundancy are more resilient to errors on one path \cite{albert_error_2000}
and allow innovations to propagate through complex contagion,
in which innovations are only adopted after multiple exposures thorugh different sources
\cite{centola_complex_2007}.

The mean path and min-cut network measures are computationally intensive,
requiring distance and minimum $st$-cut calculations for each all node pairs.
For larger projects, these calculations are impractical.
When coeditor networks were large, we employed sampling to determine mean path length and mean
min-cut.
For mean path length, source nodes were sampled, and path length was calculated to all destination nodes
from each of these.
For min-cut, node pairs were sampled.
In both cases, stratification was used to ensure the same number of nodes were were sampled from each of
12 node degree quantiles.
We estimated the error due to sampling by determining true values for a medium-sized project,
and calculating error as a function of sample-size.
Sample sizes were chosen such that relative error was below 10\%.
Even with sampling, however, it was impractical to calculate these properties for the largest projects,
so we exclude them from the analysis.
To control for bias in project size, we include several size-related variables in our models.

\subsection{Model}

We model performance and efficiency in WikiProjects using ordinary least-squares linear regression.
Each WikiProject is taken as a single observation.
The models include each project's coeditor network properties as independent variables.
We also include several project-level variables to control for confounding factors.
The C-efficiency is included in the model for performance to control for the presence of articles
that are actively being improved.
Projects with lower efficiency will have more works-in-progress and could have an artifically low
performance without controlling for efficiency.
Some variables are log-transformed to reduce heteroscedasticity.

Our models are summarized in Table \ref{tab:model}.
Mean min-cut was found to be highly correlated with degree (see Figure \ref{fig:degree-mincut}),
so we exclude min-cut to prevent collinearity.
The high correlation between mean degree and min-cut implies that in most cases
the minimum $st$-cut is simply the either set of edges from $s$ or the set of edges to $t$.
The rarity of non-trivial min-cuts suggests that WikiProject coeditor networks have very few central
bottlenecks and are thus highly decentralized.
In-degree and out-degree skewness were also highly correlated, so each dependent variable was modelled twice:
once with in-degree skewness and once with out-degree skewness.


The regression results are consistent whether in-degree or out-degree skewness is used,
although the out-degree models are slightly more significant when modelling efficiency.


\begin{figure}
\caption{
TODO
\label{fig:degree-mincut}
}
\end{figure}

\begin{table*}
\small
\begin{tabular}{lllllllll}
                               & Perf$^\dagger$ & Perf$^\dagger$ & A-Eff$^\dagger$ & A-Eff$^\dagger$ & B-Eff$^\dagger$ & B-Eff$^\dagger$ & C-Eff$^\dagger$ & C-Eff$^\dagger$ \\
\hline
Mean degree$^\dagger$          &  -0.84$^{***}$  &  -0.78$^{***}$
                               &  -0.61$^{**}$   &  -0.84$^{***}$
                               &  -0.50$^{***}$  &  -0.61$^{***}$
                               &  -0.34$^{**}$   &  -0.31$^{*}$ \\
In-degree skewness$^\dagger$   &  -0.58$^{***}$  & \+ ---      
                               &  -0.43$^{**}$   & \+ ---      
                               &  -0.19          & \+ ---      
                               &  -0.1           & \+ ---      \\
Out-degree skewness$^\dagger$  &  \+ ---         &  -0.48$^{***}$  
                               &  \+ ---         &  -0.57$^{***}$
                               &  \+ ---         &  -0.26$^{*}$
                               &  \+ ---         &  -0.066 \\
Mean path$^\dagger$            &  -0.357$^{***}$ &  -0.356$^{***}$ 
                               &  -0.026         &  -0.096
                               &  -0.020         &  -0.045
                               &  -0.092$^{**}$  &  -0.086 \\
C-eff$^\dagger$                &  -0.083$^{**}$  & -0.085$^{**}$  
                               &  \+ ---         & \+ ---      
                               &  \+ ---         & \+ ---      
                               &  \+ ---         & \+ --- \\
Connectivity                   & \+0.018         & \+0.016        
                               & \+0.081         & \+0.088
                               & \+0.126$^{***}$ & \+0.142$^{***}$
                               & \+0.080$^{**}$  & \+0.081$^{**}$ \\
Mean editors/article$^\dagger$ & \+0.37$^{***}$  & \+0.40$^{***}$ 
                               & \+0.22          & \+0.27
                               & \+0.20          & \+0.22$^{*}$
                               & \+0.076         & \+0.075 \\
Article count$^\dagger$        &  -0.32          & -0.27          
                               & \+0.63${^*}$    & \+0.70${^*}$ 
                               & \+0.76$^{**}$   & \+0.78$^{**}$
                               & \+0.67$^{***}$  & \+0.67$^{***}$ \\
Editor count$^\dagger$         & \+0.68$^{*}$    & \+0.48         
                               & \+0.74$^{*}$    & \+0.97$^{**}$
                               & \+0.60$^{*}$    & \+0.72$^{**}$
                               & \+0.52$^{*}$    & \+0.46$^{*}$ \\
Revision count$^\dagger$       & \+0.37          & \+0.39         
                               &  -0.84$^{**}$   & -0.88$^{**}$
                               &  -1.05$^{***}$  & -1.05$^{***}$
                               &  -1.00$^{***}$  & -1.00$^{***}$ \\
First assessment               & \+0.028         & \+0.058        
                               & \+0.086$^{*}$   & \+0.101$^{**}$
                               & \+0.309$^{***}$ & \+0.321$^{***}$
                               & \+0.456$^{***}$ & \+0.460$^{***}$ \\
Mean article age               &  -0.040         & -0.029         
                               &  -0.049         & -0.037
                               &  -0.019         & -0.014
                               &  -0.046$^{*}$   & -0.045$^{*}$ \\
\hline
DoF                            & 1278            & 1278          
                               & 1048            & 1048
                               & 1371            & 1371
                               & 1540            & 1540 \\
R$^2_{adj}$                    & 0.37            & 0.37          
                               & 0.15            & 0.16
                               & 0.30            & 0.30
                               & 0.43            & 0.43 \\
\end{tabular}
\caption{Standardized coefficients for OLS models.
\label{tab:model}
}
\begin{tablenotes}
\item $\dagger$ Log-transformed. * $p < 0.05$. ** $p < 0.01$. *** $p < 0.001$.
\end{tablenotes}
\end{table*}

\section{Numerical Simulation}

Intro

\subsection{Learning Strategies}

Individual learning

Social learning and iteration

Best neighbor

Conformity

Consensus

\subsection{Network family}

TODO

Base network

Duplication and rewiring

\subsection{Simulation results}

TODO

\section{Discussion}
TODO

\section{Conclusion}
TODO

\section{Acknowledgements}
I would like to thank Daniel Romero for valuable guidance and feedback;
Danielle Livneh and Karthik Ramanathan for help collecting the data sets;
Yan Chen and Tanya Rosenblat for feedback on the methodology;
and the atendees of the May 25, 2017 MIT Center for Civic Media lab meeting and
the Berkman-Klein Center's Cooperation Working Group for helpful feedback on
preliminary results.
This research was funded by the University of Michigan School of Information.

% ACM
%\bibliographystyle{ACM-Reference-Format}

% Manuscript
\bibliographystyle{acm}

\bibliography{paper} 

\end{document}
