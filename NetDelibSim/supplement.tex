\documentclass[twocolumn,10pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{times}
\usepackage{geometry}
\usepackage[ruled,vlined]{algorithm2e}

\usepackage[switch]{lineno}
\linenumbers

\newtheorem{definition}{Definition}
\newtheorem{property}{Property}
\newtheorem{claim}{Claim}

\DeclareMathOperator{\unif}{unif}
\DeclareMathOperator{\sample}{sample}
\DeclareMathOperator{\setcount}{count}
\DeclareMathOperator{\mode}{mode}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usepackage{geometry}
\geometry{left=1.5cm,right=1.5cm,top=2cm,bottom=2cm}
\setlength{\columnsep}{0.5cm}

\title{Supplementary Information: \\
Small interlocking groups improve mass deliberation\\in the presence of strong social influence}
\author{
%Edward L. Platt\\
%University of Michigan\\
%elplatt@umich.edu
%and
%Herminio Bodon\\
%Northwestern University\\
%HerminioBodon2020\\@u.northwestern.edu
%\and
%Daniel M. Romero\\
%University of Michigan\\
%drom@umich.edu
Authors Redacted \\
for \\
Double-Blind Peer-Review
}
\date{\today}

\begin{document}

\maketitle

\section{Simulation Procedure}
We begin with a network $(V,E_t)$. The vertices $V$ correspond to agents. The edges $E_t$ allow agents to exchange information with their immediate neighbors at time $t$.
Agents collaborate to find a solution $s$ from a space of solutions $\mathcal{S}$ that maximizes an objective function $Q(s)$.
We use binary strings of length $d$ as our solution space: $s \in \mathbb{Z}^d$.
At any one time $t$, each agent $v$ has exactly one preferred solution $s_{v,t}$.
We generate a tunably rugged objective function $Q(s)$ using an NK-Model \cite{kauffman_towards_1987} (see Section \ref{subsec:task}) with N=15, K=6, exp=8.

\section{Network Topologies}

We use networks to represent constraints on who talks to whom. Each agent is represented by a vertex, and two agents are able to interact when their vertices are connected by an edge. In this paper we are concerned with network deliberation, which takes place on interlock networks. Interlocks are composed of small cliques (i.e., pods) with some vertices belonging to multiple cliques. For the interlock networks in this paper, each vertex belongs to exactly one pod at a time, with pod membership periodically reassigned to achieve multiple membership. As a control, we also consider networks representing conventional deliberation.


\section{Learning Strategies}
\label{sec:learning}

% Exclude bitwise majority from paper
\iffalse

\subsection{Bitwise Majority}
Instead of evaluating entire solutions on their popularity, it is also possible to evaluate the popularity of each component of a solution, i.e., each bit in the bit string \cite{platt_network_2018}.
We refer to this strategy as {\em bitwise majority} social learning.
As with previous strategies, bitwise majority can result in ties for individual bits, which result in ties for the resulting full solution.

\begin{definition}
The bitwise majority strategy $\mathcal{L}_{bitwise}$ is defined as:
\begin{eqnarray}
S_i &=& \bigcup_{x \in S} \{x_i\} \\
\mathcal{L}_{bitwise}(S)
&=& \mode(S_1) \times \mode(S_2) \ldots \times \mode(S_d),
\end{eqnarray}
where $x_i$ is the $i$th element of the binary string $x$,
$S_i$ is a multiset, and $\mode(S)$ returns a set containing the mode or modes of $S$, and $\times$ is the Cartesian product.
\end{definition}
As with conform, the bitwise majority strategy does not incorporate any new information about solution quality.
Bitwise majority also has the notable property of being able to produce novel solutions that vary significantly from any previously seen.

% End of bitwise majority
\fi

\section{NK Model}
\label{subsec:task}
The NK Model \cite{kauffman_towards_1987} is an optimization problem particularly well-suited for modelling complex tasks. The model is used to create a fitness function $Q(s)$ over some discrete space $\mathcal{S}$, typically binary strings of a fixed length.
The model is parameterized by two variables.
The first, $N$, is the dimension of the solution space, i.e., the length of each binary string.
The second parameter, $K$, determines the ``ruggedness'' of the fitness function, i.e., the number of local maxima.
In effect, the $K$ parameter allows the complexity of the optimization problem to be tuned.
The ability to tune task complexity makes the NK model well-suited for studying the role of complexity in various settings.
The construction of an NK fitness function from a set of parameters is a stochastic process.
So particular values of $N$ and $K$ define a class of fitness functions, which can be sampled to produce a specific fitness function.

We now show the construction of the NK model fitness function.
A class of NK model fitness functions can be defined by a tuple of integer parameters
$(N, K)$ such that $N > 0$ and $0 \leq K < N$.
We begin by defining $N$ fitness contributions functions:
\begin{eqnarray}
q_i &:& \mathbb{Z}^{K+1} \rightarrow [0, 1].
\end{eqnarray}
The value of each $q_i(x)$ is chosen uniformly at random in the range $[0, 1]$ at the time the function is defined.
We also define $N$ projection operators $P_i$ which select $K+1$ bits from a length-$N$ bit string.
Each $P_i$ selects the bit at index $i$ and $K$ other indices, chosen uniformly at random at the time $P_i$ is constructed.
For a solution $s$, the $i$th fitness contribution is evaluated on the $K + 1$ bits selected by the $i$th projection: $q_i(P_i(s))$.
The value of the fitness function is the mean of all fitness contributions:
\begin{eqnarray}
Q(s) &=& \frac{1}{N}\sum_{i=1}^N q_i(P_i(s)).
\end{eqnarray}

The parameter $K$ alters the ruggedness of the fitness function by controlling the interdependence between the $q_i$.
When $K = 0$, each $q_i$ depends on a single unique index of $s$,
allowing the $q_i$ to be optimized independently.
In this case, $Q(s)$ has a single maximum: the global maximum.
However, for $K > 0$, two things happen: it becomes possible for each $q_i$ to have multiple local maxima, and some of the $q_i$ become coupled due to dependence on the same indices of $s$.
The result is that as $K$ increases, both the number of local maxima of $Q(s)$ \cite{weinberger_local_1991} and the difficulty of simultaneously optimizing the $q_i$ increase.
In other words, $Q$ becomes more rugged, and more complex.

The distribution of local maximum values is asymptotically normal for large $K$ \cite{weinberger_local_1991}.
When a skewed distribution is preferred, it is common to exponentiate the value of $Q(s)$ as a final step \cite{lazer_network_2007, barkoczi_social_2016, gomez_clustering_2019}.

\section{Supplemental Results}



\bibliography{references}
\bibliographystyle{plain}

\end{document}
